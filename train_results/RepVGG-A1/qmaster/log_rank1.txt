[2023-10-16 16:58:59 RepVGG-A1] (main.py 482): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: weak
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/datasets/ILSVRC2012
  IMG_SIZE: 320
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 32
  TEST_SIZE: 320
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 1
MODEL:
  ARCH: RepVGG-A1
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: train_results/RepVGG-A1/qmaster
PRINT_FREQ: 500
SAVE_FREQ: 10
SEED: 0
TAG: qmaster
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.05
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.0001

[2023-10-16 16:59:03 RepVGG-A1] (main.py 97): INFO Creating model:RepVGG-A1
[2023-10-16 16:59:04 RepVGG-A1] (main.py 107): INFO RepVGG(
  (stage0): RepVGGBlock(
    (nonlinearity): ReLU()
    (se): Identity()
    (dx_add): DxAdd()
    (rbr_reparam): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
  (stage1): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage2): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage3): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (4): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (5): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (6): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (7): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (8): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (9): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (10): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (11): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (12): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (13): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage4): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (linear): Linear(in_features=1280, out_features=1000, bias=True)
  (identity): Identity()
  (dx_view): DxView()
)
[2023-10-16 16:59:08 RepVGG-A1] (main.py 160): INFO number of params: 12796478
[2023-10-16 16:59:08 RepVGG-A1] (main.py 197): INFO no checkpoint found in train_results/RepVGG-A1/qmaster, ignoring auto resume
[2023-10-16 16:59:08 RepVGG-A1] (main.py 203): INFO Start training
[2023-10-16 16:59:10 RepVGG-A1] (main.py 332): INFO Train: [0/300][0/10009]	eta 4:43:56 lr 0.050000	time 1.7021 (1.7021)	loss 3.7916 (3.7916)	grad_norm 154.6503 (154.6503)	mem 4855MB
[2023-10-16 17:00:15 RepVGG-A1] (main.py 332): INFO Train: [0/300][500/10009]	eta 0:21:11 lr 0.050000	time 0.1332 (0.1337)	loss 4338.2793 (4372.9996)	grad_norm 52518.8838 (24165.7466)	mem 4855MB
[2023-10-16 17:01:24 RepVGG-A1] (main.py 482): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: weak
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/datasets/ILSVRC2012
  IMG_SIZE: 320
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 32
  TEST_SIZE: 320
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 1
MODEL:
  ARCH: RepVGG-A1
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: train_results/RepVGG-A1/qmaster
PRINT_FREQ: 10
SAVE_FREQ: 10
SEED: 0
TAG: qmaster
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.05
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.0001

[2023-10-16 17:01:28 RepVGG-A1] (main.py 97): INFO Creating model:RepVGG-A1
[2023-10-16 17:01:28 RepVGG-A1] (main.py 107): INFO RepVGG(
  (stage0): RepVGGBlock(
    (nonlinearity): ReLU()
    (se): Identity()
    (dx_add): DxAdd()
    (rbr_reparam): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
  (stage1): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage2): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage3): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (4): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (5): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (6): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (7): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (8): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (9): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (10): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (11): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (12): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (13): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage4): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (linear): Linear(in_features=1280, out_features=1000, bias=True)
  (identity): Identity()
  (dx_view): DxView()
)
[2023-10-16 17:01:33 RepVGG-A1] (main.py 160): INFO number of params: 12796478
[2023-10-16 17:01:33 RepVGG-A1] (main.py 197): INFO no checkpoint found in train_results/RepVGG-A1/qmaster, ignoring auto resume
[2023-10-16 17:01:33 RepVGG-A1] (main.py 203): INFO Start training
[2023-10-16 17:01:35 RepVGG-A1] (main.py 332): INFO Train: [0/300][0/10009]	eta 4:49:20 lr 0.050000	time 1.7345 (1.7345)	loss 3.7916 (3.7916)	grad_norm 154.6503 (154.6503)	mem 4855MB
[2023-10-16 17:01:36 RepVGG-A1] (main.py 332): INFO Train: [0/300][10/10009]	eta 0:46:47 lr 0.050000	time 0.1324 (0.2807)	loss 7354.1963 (3258.0813)	grad_norm 1070.5059 (2151.0513)	mem 4855MB
[2023-10-16 17:01:37 RepVGG-A1] (main.py 332): INFO Train: [0/300][20/10009]	eta 0:35:14 lr 0.050000	time 0.1289 (0.2117)	loss 5593.5571 (4762.9276)	grad_norm 1370.6907 (1634.2041)	mem 4855MB
[2023-10-16 17:01:39 RepVGG-A1] (main.py 332): INFO Train: [0/300][30/10009]	eta 0:31:02 lr 0.050000	time 0.1370 (0.1866)	loss 5726.0293 (5017.5938)	grad_norm 607.3977 (1420.2412)	mem 4855MB
[2023-10-16 17:01:40 RepVGG-A1] (main.py 332): INFO Train: [0/300][40/10009]	eta 0:28:58 lr 0.050000	time 0.1428 (0.1744)	loss 5739.7100 (5095.9122)	grad_norm 851.0770 (1315.2043)	mem 4855MB
[2023-10-16 17:01:41 RepVGG-A1] (main.py 332): INFO Train: [0/300][50/10009]	eta 0:27:38 lr 0.050000	time 0.1374 (0.1666)	loss 5488.7729 (5097.7088)	grad_norm 1919.8141 (1302.7334)	mem 4855MB
[2023-10-16 17:01:43 RepVGG-A1] (main.py 332): INFO Train: [0/300][60/10009]	eta 0:26:47 lr 0.050000	time 0.1380 (0.1616)	loss 4346.4668 (5007.1043)	grad_norm 3643.3702 (1548.0566)	mem 4855MB
[2023-10-16 17:01:44 RepVGG-A1] (main.py 332): INFO Train: [0/300][70/10009]	eta 0:26:06 lr 0.050000	time 0.1247 (0.1576)	loss 4661.9629 (4948.1347)	grad_norm 5592.6847 (1994.2929)	mem 4855MB
[2023-10-16 17:01:45 RepVGG-A1] (main.py 332): INFO Train: [0/300][80/10009]	eta 0:25:37 lr 0.050000	time 0.1291 (0.1548)	loss 4994.1074 (4919.5449)	grad_norm 7122.0324 (2553.7392)	mem 4855MB
[2023-10-16 17:01:47 RepVGG-A1] (main.py 332): INFO Train: [0/300][90/10009]	eta 0:25:14 lr 0.050000	time 0.1383 (0.1527)	loss 5286.0947 (4883.6809)	grad_norm 7884.5181 (3101.7619)	mem 4855MB
[2023-10-16 17:01:48 RepVGG-A1] (main.py 332): INFO Train: [0/300][100/10009]	eta 0:24:54 lr 0.050000	time 0.1275 (0.1508)	loss 4684.2134 (4841.7068)	grad_norm 8951.4875 (3630.0222)	mem 4855MB
[2023-10-16 17:01:49 RepVGG-A1] (main.py 332): INFO Train: [0/300][110/10009]	eta 0:24:39 lr 0.050000	time 0.1434 (0.1495)	loss 5189.8105 (4832.8588)	grad_norm 10300.2290 (4175.4623)	mem 4855MB
[2023-10-16 17:01:51 RepVGG-A1] (main.py 332): INFO Train: [0/300][120/10009]	eta 0:24:26 lr 0.050000	time 0.1308 (0.1483)	loss 5220.5596 (4826.4120)	grad_norm 11652.2664 (4743.5450)	mem 4855MB
[2023-10-16 17:01:52 RepVGG-A1] (main.py 332): INFO Train: [0/300][130/10009]	eta 0:24:18 lr 0.050000	time 0.1393 (0.1476)	loss 4534.9800 (4809.6892)	grad_norm 12920.3154 (5324.8554)	mem 4855MB
[2023-10-16 17:01:53 RepVGG-A1] (main.py 332): INFO Train: [0/300][140/10009]	eta 0:24:07 lr 0.050000	time 0.1374 (0.1467)	loss 4276.5669 (4788.2760)	grad_norm 13948.7734 (5905.6076)	mem 4855MB
[2023-10-16 17:01:55 RepVGG-A1] (main.py 332): INFO Train: [0/300][150/10009]	eta 0:23:57 lr 0.050000	time 0.1299 (0.1458)	loss 4537.9570 (4758.0680)	grad_norm 14763.5972 (6468.0881)	mem 4855MB
[2023-10-16 17:01:56 RepVGG-A1] (main.py 332): INFO Train: [0/300][160/10009]	eta 0:23:49 lr 0.050000	time 0.1386 (0.1451)	loss 4100.4390 (4750.1441)	grad_norm 15659.8197 (7013.2442)	mem 4855MB
[2023-10-16 17:02:07 RepVGG-A1] (main.py 482): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  PRESET: weak
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/datasets/ILSVRC2012
  IMG_SIZE: 320
  INTERPOLATION: bilinear
  NUM_WORKERS: 8
  PIN_MEMORY: true
  TEST_BATCH_SIZE: 32
  TEST_SIZE: 320
  ZIP_MODE: false
EVAL_MODE: false
LOCAL_RANK: 1
MODEL:
  ARCH: RepVGG-A1
  LABEL_SMOOTHING: 0.1
  NUM_CLASSES: 1000
  RESUME: ''
OUTPUT: train_results/RepVGG-A1/qmaster
PRINT_FREQ: 10
SAVE_FREQ: 10
SEED: 0
TAG: qmaster
TEST:
  CROP: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.005
  CLIP_GRAD: 0.0
  EMA_ALPHA: 0.0
  EMA_UPDATE_PERIOD: 8
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 0.0
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  SCALES_PATH: null
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 0.0001

[2023-10-16 17:02:11 RepVGG-A1] (main.py 97): INFO Creating model:RepVGG-A1
[2023-10-16 17:02:12 RepVGG-A1] (main.py 107): INFO RepVGG(
  (stage0): RepVGGBlock(
    (nonlinearity): ReLU()
    (se): Identity()
    (dx_add): DxAdd()
    (rbr_reparam): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
  (stage1): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage2): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage3): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (1): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (2): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (4): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (5): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (6): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (7): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (8): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (9): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (10): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (11): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (12): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (13): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (stage4): ModuleList(
    (0): RepVGGBlock(
      (nonlinearity): ReLU()
      (se): Identity()
      (dx_add): DxAdd()
      (rbr_reparam): Conv2d(256, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
  (gap): AdaptiveAvgPool2d(output_size=1)
  (linear): Linear(in_features=1280, out_features=1000, bias=True)
  (identity): Identity()
  (dx_view): DxView()
)
[2023-10-16 17:02:16 RepVGG-A1] (main.py 160): INFO number of params: 12796478
[2023-10-16 17:02:16 RepVGG-A1] (main.py 197): INFO no checkpoint found in train_results/RepVGG-A1/qmaster, ignoring auto resume
[2023-10-16 17:02:16 RepVGG-A1] (main.py 203): INFO Start training
[2023-10-16 17:02:18 RepVGG-A1] (main.py 332): INFO Train: [0/300][0/10009]	eta 4:41:36 lr 0.005000	time 1.6882 (1.6882)	loss 3.7916 (3.7916)	grad_norm 154.6503 (154.6503)	mem 4855MB
[2023-10-16 17:02:19 RepVGG-A1] (main.py 332): INFO Train: [0/300][10/10009]	eta 0:45:16 lr 0.005000	time 0.1304 (0.2717)	loss 972.5245 (495.3313)	grad_norm 1480.5084 (2732.7616)	mem 4855MB
[2023-10-16 17:02:20 RepVGG-A1] (main.py 332): INFO Train: [0/300][20/10009]	eta 0:34:02 lr 0.005000	time 0.1279 (0.2044)	loss 1978.0837 (951.8935)	grad_norm 1395.3770 (2112.5448)	mem 4855MB
[2023-10-16 17:02:22 RepVGG-A1] (main.py 332): INFO Train: [0/300][30/10009]	eta 0:30:05 lr 0.005000	time 0.1347 (0.1809)	loss 2599.3862 (1373.6666)	grad_norm 1357.4746 (1862.5277)	mem 4855MB
[2023-10-16 17:02:23 RepVGG-A1] (main.py 332): INFO Train: [0/300][40/10009]	eta 0:27:58 lr 0.005000	time 0.1261 (0.1683)	loss 2743.2393 (1701.5066)	grad_norm 1336.6485 (1731.8392)	mem 4855MB
[2023-10-16 17:02:24 RepVGG-A1] (main.py 332): INFO Train: [0/300][50/10009]	eta 0:26:44 lr 0.005000	time 0.1380 (0.1611)	loss 3314.2397 (1920.1224)	grad_norm 1332.8181 (1643.6022)	mem 4855MB
[2023-10-16 17:02:26 RepVGG-A1] (main.py 332): INFO Train: [0/300][60/10009]	eta 0:25:54 lr 0.005000	time 0.1350 (0.1562)	loss 2980.8916 (2114.9282)	grad_norm 1331.9879 (1580.8937)	mem 4855MB
[2023-10-16 17:02:27 RepVGG-A1] (main.py 332): INFO Train: [0/300][70/10009]	eta 0:25:17 lr 0.005000	time 0.1320 (0.1526)	loss 3395.9380 (2300.7906)	grad_norm 1330.1197 (1535.9794)	mem 4855MB
[2023-10-16 17:02:28 RepVGG-A1] (main.py 332): INFO Train: [0/300][80/10009]	eta 0:24:48 lr 0.005000	time 0.1326 (0.1499)	loss 4715.2100 (2470.4297)	grad_norm 1334.3802 (1506.8138)	mem 4855MB
[2023-10-16 17:02:30 RepVGG-A1] (main.py 332): INFO Train: [0/300][90/10009]	eta 0:24:26 lr 0.005000	time 0.1254 (0.1479)	loss 3332.5994 (2625.0388)	grad_norm 1336.7254 (1484.7779)	mem 4855MB
[2023-10-16 17:02:31 RepVGG-A1] (main.py 332): INFO Train: [0/300][100/10009]	eta 0:24:07 lr 0.005000	time 0.1192 (0.1460)	loss 4575.8447 (2763.4641)	grad_norm 1357.4315 (1467.5626)	mem 4855MB
[2023-10-16 17:02:32 RepVGG-A1] (main.py 332): INFO Train: [0/300][110/10009]	eta 0:23:53 lr 0.005000	time 0.1237 (0.1448)	loss 4633.0674 (2920.9869)	grad_norm 1338.1747 (1447.0514)	mem 4855MB
[2023-10-16 17:02:34 RepVGG-A1] (main.py 332): INFO Train: [0/300][120/10009]	eta 0:23:40 lr 0.005000	time 0.1291 (0.1436)	loss 4783.6689 (3061.9016)	grad_norm 1398.4171 (1429.7562)	mem 4855MB
[2023-10-16 17:02:35 RepVGG-A1] (main.py 332): INFO Train: [0/300][130/10009]	eta 0:23:29 lr 0.005000	time 0.1245 (0.1427)	loss 4471.3125 (3231.4801)	grad_norm 1402.9324 (1424.2864)	mem 4855MB
[2023-10-16 17:02:36 RepVGG-A1] (main.py 332): INFO Train: [0/300][140/10009]	eta 0:23:20 lr 0.005000	time 0.1291 (0.1419)	loss 4813.6494 (3354.0339)	grad_norm 1125.6452 (1412.0005)	mem 4855MB
[2023-10-16 17:02:37 RepVGG-A1] (main.py 332): INFO Train: [0/300][150/10009]	eta 0:23:10 lr 0.005000	time 0.1302 (0.1411)	loss 6941.7852 (3521.1937)	grad_norm 1457.9271 (1400.1868)	mem 4855MB
[2023-10-16 17:02:39 RepVGG-A1] (main.py 332): INFO Train: [0/300][160/10009]	eta 0:23:03 lr 0.005000	time 0.1438 (0.1405)	loss 4717.9312 (3633.4148)	grad_norm 928.0259 (1385.1072)	mem 4855MB
[2023-10-16 17:02:40 RepVGG-A1] (main.py 332): INFO Train: [0/300][170/10009]	eta 0:22:56 lr 0.005000	time 0.1275 (0.1399)	loss 6023.2202 (3745.7870)	grad_norm 1740.9606 (1380.3643)	mem 4855MB
[2023-10-16 17:02:41 RepVGG-A1] (main.py 332): INFO Train: [0/300][180/10009]	eta 0:22:49 lr 0.005000	time 0.1267 (0.1394)	loss 4290.4385 (3844.0334)	grad_norm 3186.8053 (1442.6894)	mem 4855MB
[2023-10-16 17:02:43 RepVGG-A1] (main.py 332): INFO Train: [0/300][190/10009]	eta 0:22:45 lr 0.005000	time 0.1266 (0.1391)	loss 6462.8525 (3931.5142)	grad_norm 4158.9831 (1565.0751)	mem 4855MB
[2023-10-16 17:02:44 RepVGG-A1] (main.py 332): INFO Train: [0/300][200/10009]	eta 0:22:40 lr 0.005000	time 0.1323 (0.1387)	loss 5441.7461 (3999.4843)	grad_norm 6033.8706 (1745.3166)	mem 4855MB
[2023-10-16 17:02:45 RepVGG-A1] (main.py 332): INFO Train: [0/300][210/10009]	eta 0:22:36 lr 0.005000	time 0.1277 (0.1384)	loss 4892.3159 (4069.1408)	grad_norm 7917.6027 (1997.6680)	mem 4855MB
[2023-10-16 17:02:47 RepVGG-A1] (main.py 332): INFO Train: [0/300][220/10009]	eta 0:22:31 lr 0.005000	time 0.1281 (0.1380)	loss 4983.3003 (4116.5584)	grad_norm 9793.2516 (2312.2456)	mem 4855MB
[2023-10-16 17:02:48 RepVGG-A1] (main.py 332): INFO Train: [0/300][230/10009]	eta 0:22:26 lr 0.005000	time 0.1267 (0.1376)	loss 5575.7344 (4176.6791)	grad_norm 11645.0769 (2680.3198)	mem 4855MB
[2023-10-16 17:02:49 RepVGG-A1] (main.py 332): INFO Train: [0/300][240/10009]	eta 0:22:22 lr 0.005000	time 0.1266 (0.1374)	loss 5130.1455 (4216.3323)	grad_norm 13471.1611 (3094.1191)	mem 4855MB
[2023-10-16 17:02:51 RepVGG-A1] (main.py 332): INFO Train: [0/300][250/10009]	eta 0:22:17 lr 0.005000	time 0.1299 (0.1370)	loss 7409.0918 (4283.1243)	grad_norm 15124.7941 (3546.2407)	mem 4855MB
[2023-10-16 17:02:52 RepVGG-A1] (main.py 332): INFO Train: [0/300][260/10009]	eta 0:22:12 lr 0.005000	time 0.1282 (0.1367)	loss 4810.4922 (4317.9828)	grad_norm 16794.8499 (4024.4089)	mem 4855MB
[2023-10-16 17:02:53 RepVGG-A1] (main.py 332): INFO Train: [0/300][270/10009]	eta 0:22:09 lr 0.005000	time 0.1339 (0.1365)	loss 4932.3086 (4366.4774)	grad_norm 18434.3399 (4529.1815)	mem 4855MB
[2023-10-16 17:02:54 RepVGG-A1] (main.py 332): INFO Train: [0/300][280/10009]	eta 0:22:06 lr 0.005000	time 0.1305 (0.1363)	loss 6640.9150 (4419.2793)	grad_norm 19987.5819 (5054.7171)	mem 4855MB
[2023-10-16 17:02:56 RepVGG-A1] (main.py 332): INFO Train: [0/300][290/10009]	eta 0:22:03 lr 0.005000	time 0.1290 (0.1362)	loss 5106.6064 (4457.3572)	grad_norm 21447.7733 (5595.6894)	mem 4855MB
[2023-10-16 17:02:57 RepVGG-A1] (main.py 332): INFO Train: [0/300][300/10009]	eta 0:21:59 lr 0.005000	time 0.1279 (0.1359)	loss 4986.3916 (4487.9057)	grad_norm 22827.1809 (6147.7541)	mem 4855MB
[2023-10-16 17:02:58 RepVGG-A1] (main.py 332): INFO Train: [0/300][310/10009]	eta 0:21:56 lr 0.005000	time 0.1430 (0.1357)	loss 4697.0234 (4526.4190)	grad_norm 24138.4524 (6707.4159)	mem 4855MB
[2023-10-16 17:03:00 RepVGG-A1] (main.py 332): INFO Train: [0/300][320/10009]	eta 0:21:53 lr 0.005000	time 0.1380 (0.1356)	loss 4922.4268 (4560.2062)	grad_norm 25398.9099 (7272.1341)	mem 4855MB
[2023-10-16 17:03:01 RepVGG-A1] (main.py 332): INFO Train: [0/300][330/10009]	eta 0:21:50 lr 0.005000	time 0.1280 (0.1354)	loss 4084.7600 (4577.9644)	grad_norm 26626.3924 (7840.2441)	mem 4855MB
[2023-10-16 17:03:02 RepVGG-A1] (main.py 332): INFO Train: [0/300][340/10009]	eta 0:21:48 lr 0.005000	time 0.1348 (0.1353)	loss 5241.3818 (4604.5459)	grad_norm 27833.5925 (8410.6841)	mem 4855MB
[2023-10-16 17:03:04 RepVGG-A1] (main.py 332): INFO Train: [0/300][350/10009]	eta 0:21:45 lr 0.005000	time 0.1217 (0.1352)	loss 4846.7734 (4645.1937)	grad_norm 29032.5640 (8982.8524)	mem 4855MB
[2023-10-16 17:03:05 RepVGG-A1] (main.py 332): INFO Train: [0/300][360/10009]	eta 0:21:43 lr 0.005000	time 0.1380 (0.1351)	loss 5432.9961 (4658.3381)	grad_norm 30233.1718 (9556.5340)	mem 4855MB
[2023-10-16 17:03:06 RepVGG-A1] (main.py 332): INFO Train: [0/300][370/10009]	eta 0:21:40 lr 0.005000	time 0.1356 (0.1350)	loss 6488.7021 (4687.1737)	grad_norm 31436.6686 (10131.6851)	mem 4855MB
[2023-10-16 17:03:08 RepVGG-A1] (main.py 332): INFO Train: [0/300][380/10009]	eta 0:21:38 lr 0.005000	time 0.1339 (0.1348)	loss 4684.3750 (4707.7727)	grad_norm 32646.3355 (10708.3184)	mem 4855MB
[2023-10-16 17:03:09 RepVGG-A1] (main.py 332): INFO Train: [0/300][390/10009]	eta 0:21:35 lr 0.005000	time 0.1269 (0.1347)	loss 5698.8613 (4743.5788)	grad_norm 33862.2288 (11286.4737)	mem 4855MB
[2023-10-16 17:03:10 RepVGG-A1] (main.py 332): INFO Train: [0/300][400/10009]	eta 0:21:33 lr 0.005000	time 0.1330 (0.1346)	loss 6182.0703 (4773.9889)	grad_norm 35086.6434 (11866.2404)	mem 4855MB
[2023-10-16 17:03:11 RepVGG-A1] (main.py 332): INFO Train: [0/300][410/10009]	eta 0:21:31 lr 0.005000	time 0.1386 (0.1345)	loss 5456.6694 (4790.9008)	grad_norm 36317.2185 (12447.6625)	mem 4855MB
[2023-10-16 17:03:13 RepVGG-A1] (main.py 332): INFO Train: [0/300][420/10009]	eta 0:21:29 lr 0.005000	time 0.1324 (0.1344)	loss 6670.1836 (4815.0630)	grad_norm 37554.7935 (13030.7706)	mem 4855MB
[2023-10-16 17:03:14 RepVGG-A1] (main.py 332): INFO Train: [0/300][430/10009]	eta 0:21:27 lr 0.005000	time 0.1355 (0.1344)	loss 6026.9961 (4824.1511)	grad_norm 38798.9497 (13615.6510)	mem 4855MB
[2023-10-16 17:03:15 RepVGG-A1] (main.py 332): INFO Train: [0/300][440/10009]	eta 0:21:25 lr 0.005000	time 0.1381 (0.1343)	loss 3931.4531 (4829.8451)	grad_norm 40053.6334 (14202.3417)	mem 4855MB
[2023-10-16 17:03:17 RepVGG-A1] (main.py 332): INFO Train: [0/300][450/10009]	eta 0:21:23 lr 0.005000	time 0.1432 (0.1342)	loss 4920.6836 (4845.2851)	grad_norm 41314.7393 (14790.9152)	mem 4855MB
[2023-10-16 17:03:18 RepVGG-A1] (main.py 332): INFO Train: [0/300][460/10009]	eta 0:21:20 lr 0.005000	time 0.1388 (0.1341)	loss 6674.7891 (4865.7023)	grad_norm 42581.1185 (15381.3539)	mem 4855MB
[2023-10-16 17:03:19 RepVGG-A1] (main.py 332): INFO Train: [0/300][470/10009]	eta 0:21:18 lr 0.005000	time 0.1439 (0.1341)	loss 5211.8506 (4878.1047)	grad_norm 43853.5564 (15973.7091)	mem 4855MB
[2023-10-16 17:03:21 RepVGG-A1] (main.py 332): INFO Train: [0/300][480/10009]	eta 0:21:16 lr 0.005000	time 0.1267 (0.1340)	loss 7006.2798 (4896.7979)	grad_norm 45133.2951 (16567.9623)	mem 4855MB
[2023-10-16 17:03:22 RepVGG-A1] (main.py 332): INFO Train: [0/300][490/10009]	eta 0:21:14 lr 0.005000	time 0.1346 (0.1339)	loss 6016.2793 (4898.4282)	grad_norm 46414.6079 (17164.0839)	mem 4855MB
[2023-10-16 17:03:23 RepVGG-A1] (main.py 332): INFO Train: [0/300][500/10009]	eta 0:21:12 lr 0.005000	time 0.1239 (0.1338)	loss 4893.1318 (4901.7554)	grad_norm 47704.2761 (17762.0656)	mem 4855MB
[2023-10-16 17:03:24 RepVGG-A1] (main.py 332): INFO Train: [0/300][510/10009]	eta 0:21:10 lr 0.005000	time 0.1363 (0.1337)	loss 6437.3213 (4909.0746)	grad_norm 48999.4446 (18361.9472)	mem 4855MB
[2023-10-16 17:03:26 RepVGG-A1] (main.py 332): INFO Train: [0/300][520/10009]	eta 0:21:08 lr 0.005000	time 0.1405 (0.1337)	loss 4591.9478 (4913.1693)	grad_norm 50297.5148 (18963.6942)	mem 4855MB
[2023-10-16 17:03:27 RepVGG-A1] (main.py 332): INFO Train: [0/300][530/10009]	eta 0:21:06 lr 0.005000	time 0.1256 (0.1336)	loss 4540.9805 (4929.7399)	grad_norm 51598.0154 (19567.2436)	mem 4855MB
[2023-10-16 17:03:28 RepVGG-A1] (main.py 332): INFO Train: [0/300][540/10009]	eta 0:21:04 lr 0.005000	time 0.1201 (0.1335)	loss 6074.3672 (4941.3240)	grad_norm 52903.8441 (20172.5723)	mem 4855MB
[2023-10-16 17:03:30 RepVGG-A1] (main.py 332): INFO Train: [0/300][550/10009]	eta 0:21:02 lr 0.005000	time 0.1263 (0.1335)	loss 5380.3027 (4956.1337)	grad_norm 54214.1143 (20779.6805)	mem 4855MB
[2023-10-16 17:03:31 RepVGG-A1] (main.py 332): INFO Train: [0/300][560/10009]	eta 0:21:01 lr 0.005000	time 0.1308 (0.1335)	loss 6060.9902 (4966.6736)	grad_norm 55527.4782 (21388.5317)	mem 4855MB
[2023-10-16 17:03:32 RepVGG-A1] (main.py 332): INFO Train: [0/300][570/10009]	eta 0:20:59 lr 0.005000	time 0.1242 (0.1334)	loss 7022.0635 (4977.6453)	grad_norm 56839.4863 (21999.0394)	mem 4855MB
[2023-10-16 17:03:34 RepVGG-A1] (main.py 332): INFO Train: [0/300][580/10009]	eta 0:20:57 lr 0.005000	time 0.1242 (0.1334)	loss 6692.4941 (4985.3191)	grad_norm 58156.5102 (22611.1635)	mem 4855MB
[2023-10-16 17:03:35 RepVGG-A1] (main.py 332): INFO Train: [0/300][590/10009]	eta 0:20:55 lr 0.005000	time 0.1293 (0.1333)	loss 6309.6650 (4991.6146)	grad_norm 59474.3662 (23224.8600)	mem 4855MB
[2023-10-16 17:03:36 RepVGG-A1] (main.py 332): INFO Train: [0/300][600/10009]	eta 0:20:53 lr 0.005000	time 0.1215 (0.1332)	loss 5747.2729 (5000.0454)	grad_norm 60795.7765 (23840.0911)	mem 4855MB
[2023-10-16 17:03:38 RepVGG-A1] (main.py 332): INFO Train: [0/300][610/10009]	eta 0:20:51 lr 0.005000	time 0.1324 (0.1332)	loss 5827.2983 (5002.5120)	grad_norm 62118.6985 (24456.8432)	mem 4855MB
[2023-10-16 17:03:39 RepVGG-A1] (main.py 332): INFO Train: [0/300][620/10009]	eta 0:20:49 lr 0.005000	time 0.1356 (0.1331)	loss 5171.8447 (5012.0964)	grad_norm 63444.3744 (25075.0436)	mem 4855MB
[2023-10-16 17:03:40 RepVGG-A1] (main.py 332): INFO Train: [0/300][630/10009]	eta 0:20:48 lr 0.005000	time 0.1322 (0.1331)	loss 5846.7563 (5016.9716)	grad_norm 64769.6597 (25694.6682)	mem 4855MB
[2023-10-16 17:03:41 RepVGG-A1] (main.py 332): INFO Train: [0/300][640/10009]	eta 0:20:46 lr 0.005000	time 0.1334 (0.1330)	loss 5309.0698 (5024.3733)	grad_norm 66096.5272 (26315.6295)	mem 4855MB
[2023-10-16 17:03:43 RepVGG-A1] (main.py 332): INFO Train: [0/300][650/10009]	eta 0:20:44 lr 0.005000	time 0.1361 (0.1330)	loss 5613.3965 (5034.5747)	grad_norm 67428.5000 (26937.9579)	mem 4855MB
[2023-10-16 17:03:44 RepVGG-A1] (main.py 332): INFO Train: [0/300][660/10009]	eta 0:20:42 lr 0.005000	time 0.1203 (0.1329)	loss 5439.9932 (5042.1769)	grad_norm 68762.5276 (27561.6206)	mem 4855MB
[2023-10-16 17:03:45 RepVGG-A1] (main.py 332): INFO Train: [0/300][670/10009]	eta 0:20:40 lr 0.005000	time 0.1303 (0.1329)	loss 6065.5088 (5045.5480)	grad_norm 70097.4847 (28186.5913)	mem 4855MB
[2023-10-16 17:03:47 RepVGG-A1] (main.py 332): INFO Train: [0/300][680/10009]	eta 0:20:39 lr 0.005000	time 0.1333 (0.1328)	loss 6801.3452 (5053.3621)	grad_norm 71430.4498 (28812.7918)	mem 4855MB
[2023-10-16 17:03:48 RepVGG-A1] (main.py 332): INFO Train: [0/300][690/10009]	eta 0:20:37 lr 0.005000	time 0.1306 (0.1328)	loss 3426.2727 (5055.3814)	grad_norm 72765.5633 (29440.1842)	mem 4855MB
